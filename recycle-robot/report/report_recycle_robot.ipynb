{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98279ee9",
   "metadata": {},
   "source": [
    "# APRENDIZADO POR REFOR√áO\n",
    "### **TAREFA 1**: Algoritmo *Temporal Difference (TD)* para o problema do rob√¥ de reciclagem\n",
    "### **ALUNOS**: \n",
    "- ### Alex J√∫nio Maia de Oliveira (*Matr√≠cula: 241708028*)\n",
    "- ### Jo√£o Pedro Jer√¥nimo de Oliveira (*Matr√≠cula: 241708024*) \n",
    "- ### Thalis Ambrosim Falqueto (*Matr√≠cula: 241708048*)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61260b43",
   "metadata": {},
   "source": [
    "[Introdu√ß√£o](#introdu√ß√£o)\n",
    "\n",
    "[Metodologia](#metodologia)\n",
    "\n",
    "[Resultados](#resultados)\n",
    "\n",
    "[Conclus√£o](#conclus√£o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42763272",
   "metadata": {},
   "source": [
    "## Introdu√ß√£o\n",
    "\n",
    "O problema do rob√¥ m√≥vel de reciclagem √© um exemplo cl√°ssico da aprendizagem por refor√ßo. Neste projeto, implementamos o problema utilizando o algoritmo *Temporal Difference Learning*, baseado na ideia de que um agente pode aprender a partir de suas pr√≥prias a√ß√µes e das recompensas ou penalidades que recebe em fun√ß√£o dessas a√ß√µes, ou seja, o agente aprende a partir da diferen√ßa entre as recompensas esperadas e as recompensas reais obtidas (erro temporal).\n",
    "\n",
    "Abaixo est√° uma apresenta√ß√£o breve do problema do rob√¥ m√≥vel, motivador desse projeto:\n",
    "\n",
    "\"Um rob√¥ m√≥vel tem a tarefa de coletar latas de refrigerante vazias em um ambiente de escrit√≥rio. Ele possui sensores para detectar latas, al√©m de um bra√ßo e uma garra capazes de peg√°-las e coloc√°-las em um compartimento interno; o rob√¥ funciona com uma bateria recarreg√°vel. O sistema de controle do rob√¥ possui componentes para interpretar informa√ß√µes sensoriais, para navega√ß√£o e para controlar o bra√ßo e a garra. Decis√µes de alto n√≠vel sobre como procurar por latas s√£o tomadas por um agente de reinforcement learning, com base no n√≠vel atual de carga da bateria.\n",
    "\n",
    "Para simplificar o exemplo, assumimos que apenas dois n√≠veis de carga podem ser distinguidos, formando um pequeno conjunto de estados $S=\\{high, low\\}$. Em cada estado, o agente pode decidir entre:\n",
    "\n",
    "- *search*: procurar ativamente por uma lata durante um certo per√≠odo de tempo,\n",
    "- *wait*: permanecer parado e esperar que algu√©m lhe traga uma lata, ou\n",
    "- *recharge*: retornar √† sua base para recarregar a bateria.\n",
    "\n",
    "Quando o n√≠vel de energia est√° high, recarregar seria sempre uma decis√£o tola, portanto n√£o inclu√≠mos essa op√ß√£o no conjunto de a√ß√µes para esse estado. Assim, os conjuntos de a√ß√µes s√£o: $A(high)=\\{search, wait\\}$ e $A(low)=\\{search, wait, recharge\\}$.\n",
    "\n",
    "As *rewards* s√£o geralmente zero, mas tornam-se positivas quando o rob√¥ consegue uma lata vazia, ou grandes e negativas caso a bateria se esgote completamente. A melhor forma de encontrar latas √© fazendo *search*, mas isso consome a bateria do rob√¥, enquanto *wait* n√£o consome. Sempre que o rob√¥ estiver em search, existe a possibilidade de a bateria se esgotar. Nesse caso, o rob√¥ deve desligar-se e esperar para ser resgatado (produzindo uma baixa *reward*).\n",
    "\n",
    "Se o n√≠vel de energia estiver *high*, ent√£o um per√≠odo de *search* pode ser conclu√≠do sem risco de esgotar a bateria. Um per√≠odo de *search* iniciado com n√≠vel *high* mant√©m o n√≠vel *high* com probabilidade $Œ±$ e o reduz para *low* com probabilidade $1-\\alpha$. Por outro lado, um per√≠odo de *search* iniciado quando o n√≠vel de energia est√° *low* mant√©m-no *low* com probabilidade $ùõΩ$ e esgota a bateria com probabilidade $1 - ùõΩ$. Nesse √∫ltimo caso, o rob√¥ deve ser resgatado, e a bateria √© ent√£o recarregada para *high*.\n",
    "\n",
    "Cada lata coletada pelo rob√¥ conta como uma unidade de *reward*, enquanto uma penalidade de $‚àí3$ ocorre sempre que o rob√¥ precisa ser resgatado. Sejam $r_{search}$ e $r_{wait}$, com $r_{search}$ > $r_{wait}$, respectivamente o n√∫mero esperado de latas que o rob√¥ coleta (e, portanto, a *reward* esperada) enquanto faz *search* e enquanto faz *wait*. Por fim, suponha que nenhuma lata possa ser coletada durante o retorno para *recharge*, nem em um passo no qual a bateria se esgota.\n",
    "\n",
    "Esse sistema √©, portanto, um *finite MDP*, e podemos escrever as probabilidades de transi√ß√£o e as *expected rewards*, com din√¢micas indicadas na tabela √† esquerda.\"\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"figures/image.png\" alt=\"Diagrama\" width=\"500\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b79deb",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  *Figura 1. Diagrama dos agentes, das recompensas e do ambiente do problema*\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef11d3e6",
   "metadata": {},
   "source": [
    "## Metodologia\n",
    "\n",
    "Com o objetivo de otimizar o tempo de desenvolvimento e facilitar a constru√ß√£o do rob√¥, dividimos o projeto em tr√™s partes principais:\n",
    "\n",
    "- **Ambiente** - Respons√°vel por processar as recompensas de acordo com os estados e com as intera√ß√µes do rob√¥, al√©m de calibrar as probabilidades $Œ±$ e $Œ≤$ e a taxa de aprendizado do agente (desenvolvido por Jo√£o Pedro).\n",
    "- **Agente ou Rob√¥** - Respons√°vel por administrar as probabilidades de cada estado e escolher e aprender a pol√≠tica para obter a maior recompensa poss√≠vel, de acordo com uma taxa de aprendizado (desenvolvido por Thalis Ambrosim).\n",
    "- **Gr√°ficos** - Respons√°vel por produzir os gr√°ficos das recompensas acumuladas juntamente com a curva acumulada m√©dia, do mapa de calor das probabilidades da pol√≠tica √≥tima do rob√¥ e das curvas m√©dias acumuladas de acordo com taxas de aprendizado diferentes (desenvolvido por Alex J√∫nio).\n",
    "\n",
    "Al√©m das vertentes supracitadas, criamos uma interface din√¢mica e mut√°vel para acompanhar o treinamento do rob√¥ de reciclagem com diferentes par√¢metros (probabilidade $Œ±$ e $Œ≤$, taxa de aprendizado e n√∫mero de √©pocas). Ao decorrer da estrutura√ß√£o da organiza√ß√£o do projeto, decidimos modularizar o c√≥digo nas seguintes pastas:\n",
    "\n",
    "```bash\n",
    "|--config  # Pasta que cont√©m algumas configura√ß√µes de ambiente, para facilitar o c√≥digo\n",
    "|--figures # Pasta que guarda os plots gerados pelo c√≥digo\n",
    "|--report  # Pasta que guarda o relat√≥rio do projeto\n",
    "|    |--figures  # Pasta que armazena as figuras utilizadas pelo relat√≥rio\n",
    "|    |-- report_recycle_robot.ipynb\n",
    "|--rewards  # Pasta que cont√©m o arquivo rewards.txt, que guarda a recompensa total ao final de um per√≠odo de E √©pocas\n",
    "|    |-- rewards.txt\n",
    "|--src  # Pasta que cont√©m o c√≥digo fonte do problema do rob√¥ de reciclagem\n",
    "|    |-- ambient.py\n",
    "|    |-- plot_funcs.py\n",
    "|    |-- robot.py\n",
    "|--main.py  # Arquivo que executa a aprendizagem do rob√¥, armazena os plots e gera o arquivo rewards.txt\n",
    "```\n",
    "\n",
    "Em adi√ß√£o, dados os valores $Œ±$ (probabilidade do rob√¥ come√ßar e terminar uma pesquisa com a bateria alta)e $Œ≤$ (probabilidade do rob√¥ come√ßar e terminar uma pesquisa com a bateria baixa), achamos interessandte implementarmos uma maneira das probabilidades decairem conforme¬†o¬†tempo da pesquisa¬†passa (seguindo uma distribui√ß√£o Exponencial), simulando o tempo de vida bateria do rob√¥.\n",
    "\n",
    "Por fim, dadas diferentes taxas de aprendizagem, calculamos a curva m√©dia das recompensas acumuladas para observar qual taxa gera a melhor evolu√ß√£o m√©dia do rob√¥ de acordo com a quantidade de √©pocas e as probabilidades $\\alpha$ e $\\beta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9d0cdf",
   "metadata": {},
   "source": [
    "## Resultados\n",
    "\n",
    "Ao fazer v√°rias rodadas de teste alterando os par√¢metros do rob√¥, observamos que:\n",
    "- o valor esperado das recompensas recompensas gira em torno de $0.8$ (em que a curva m√©dia da pol√≠tica √≥tima fica mais perto desse valor) e converge quando o n√∫mero de √©pocas tende ao infinito.\n",
    "- o valor acumulado das recompensas da pol√≠tica √≥tima gira em torno de $800$ ao final das √©pocas.\n",
    "- na maior parte das vezes (independentemente dos valores de $Œ±$, $Œ≤$, do n√∫mero de √©pocas e da taxa de aprendizado), quando o rob√¥ est√° com a bateria alta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d2a2a5",
   "metadata": {},
   "source": [
    "## Conclus√£o"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
